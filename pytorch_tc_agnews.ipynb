{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pytorch_sentiment_analysis.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOHLAlZJ5ApD0/k2CjfCUCB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shraddha-an/nlp/blob/main/pytorch_tc_agnews.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3evNaZYtx93C"
      },
      "source": [
        "### **Text Classification tutorial with Torch Text**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dVh6OMxzf-0Y"
      },
      "source": [
        "## **I. Data Preprocessing**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6vRQxprVfkJU"
      },
      "source": [
        "### **1) Importing Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "flq69kb5xdII"
      },
      "source": [
        "# Importing the libraries\n",
        "import torch\n",
        "from torchtext.datasets import AG_NEWS\n",
        "\n",
        "from collections import Counter"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Z8UVSDnfpbj"
      },
      "source": [
        "### **2) Buidling vocabulary**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AAcYaVqeyW15"
      },
      "source": [
        "# Tokenizing the texts and building a vocabulary object out of the training text\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "\n",
        "tokenizer = get_tokenizer('basic_english')\n",
        "count = Counter()\n",
        "\n",
        "# Importing training data\n",
        "train_data = AG_NEWS(split = 'train')\n",
        "\n",
        "# Tokenizing the sentences and updating the Counter object with the words\n",
        "for (label, text) in train_data:\n",
        "  \n",
        "  # Passing sentences through the tokenizer & passing the tokens to the counter\n",
        "  count.update(tokenizer(text))\n",
        "\n",
        "# Passing the counter object & minimum frequency arguments to the Vocab class to build the vocab object\n",
        "from torchtext.vocab import Vocab\n",
        "vocab = Vocab(counter = count, min_freq = 1)\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z59DI2VQ3h_x",
        "outputId": "c7b3f0a4-d4da-4dc2-9a88-42e36e161016"
      },
      "source": [
        "# Playing around with the vocab object\n",
        "sent = list(\"I like the song Bolero very much. I am also interested in Delta Force.\".split())\n",
        "\n",
        "s = list(\"here is an example\".split())\n",
        "[vocab[i] for i in sent]"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 318, 3, 3375, 0, 1168, 0, 0, 1914, 457, 4846, 8, 0, 0]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fvmBTkWffta-"
      },
      "source": [
        "### **3) Text Preprocessing and Label Pipelines**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i6RpqpdE4wm8"
      },
      "source": [
        "# Building the text processing pipeline -> tokenizer + converting tokens to integers.\n",
        "text_pipeline = lambda x: [vocab[token] for token in tokenizer(x)]\n",
        "\n",
        "# Label pipeline\n",
        "label_pipeline = lambda x: int(x) - 1"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R-XcT2bf723s",
        "outputId": "13184cfd-b323-437b-de2d-94681e60209c"
      },
      "source": [
        "text_pipeline('Delta force baby')\n",
        "label_pipeline(3)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kGbLOswagFnZ"
      },
      "source": [
        "### **4) Creating the DataLoaders**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mNANGySW9GLt"
      },
      "source": [
        "# Building a DataLoader object from the iterable dataset.\n",
        "# Before sending to the model, collate_fn function works on a batch of samples generated from DataLoader. \n",
        "# The input to collate_fn is a batch of data with the batch size in DataLoader, \n",
        "# and collate_fn processes them according to the data processing pipelines declared previously.\n",
        "\n",
        "# Setting device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Defining the collate function\n",
        "def collate_batch(batch):\n",
        "  label_list, text_list, offset = [], [], [0]\n",
        "\n",
        "  for (lbl, txt) in batch:\n",
        "    label_list.append(label_pipeline(lbl))\n",
        "    processed_text = torch.tensor(text_pipeline(txt), dtype = torch.int64)\n",
        "    text_list.append(processed_text)\n",
        "    offset.append(processed_text.size(0))\n",
        "\n",
        "  # Converting the lists to torch tensors\n",
        "  label_list = torch.tensor(label_list, dtype = torch.int64)\n",
        "  offset = torch.tensor(offset[:-1]).cumsum(dim = 0)\n",
        "  text_list = torch.cat(text_list) \n",
        "  return label_list.to(device), text_list.to(device), offset.to(device)\n",
        "\n",
        "# Creating the dataloader object\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "data = DataLoader(train_data, batch_size = 8, shuffle = False, collate_fn = collate_batch)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1E4fiq2vgJM8"
      },
      "source": [
        "## **II. Model Definition**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ybjPiZJR_4h9"
      },
      "source": [
        "# Importing torch libraries to define the model class\n",
        "import torch.nn as nn, torch.nn.functional as F\n",
        "\n",
        "# Defining my Text Classification model\n",
        "class TextClassification(nn.Module):\n",
        "  \n",
        "  # Defining the layer architecture in the constructor\n",
        "  def __init__(self, vocab_size, embed_dim, num_classes):\n",
        "    # Calling the parent class constructors\n",
        "    super(TextClassification, self).__init__()\n",
        "\n",
        "    # Defining my layers\n",
        "    self.embed = nn.EmbeddingBag(num_embeddings = vocab_size, embedding_dim = embed_dim, sparse = True)\n",
        "    self.fc = nn.Linear(in_features = embed_dim, out_features = num_classes)\n",
        "\n",
        "    # Calling the function to initialize weights after the layers have been defined\n",
        "    self.init_weights()\n",
        "\n",
        "  # Defining a function to initialize weights\n",
        "  def init_weights(self):\n",
        "    # Initializing the weights of the embedding layer & the fc layer with values drawn from a uniform distribution of (-k, k)\n",
        "    initrange = 0.5\n",
        "    self.embed.weight.data.uniform_(-initrange, initrange)\n",
        "    self.fc.weight.data.uniform_(-initrange, initrange)\n",
        "    \n",
        "    # Initializing the bias of the fc layer to zero\n",
        "    self.fc.bias.data.zero_()\n",
        "\n",
        "  # Defining the forward pass computations\n",
        "  def forward(self, text, offset):\n",
        "    output = self.embed(text, offset)\n",
        "    \n",
        "    return self.fc(output)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ThZuqvrDJe6S",
        "outputId": "a7473ead-8b81-4600-ee0e-790a78a8c347"
      },
      "source": [
        "# Creating an object of the TextClassificationModel\n",
        "train_iter  = AG_NEWS(split = 'train')\n",
        "num_classes = len(set([label for (label, text) in train_iter]))\n",
        "vocab_size = len(vocab)\n",
        "embedding_size = 64\n",
        "\n",
        "model = TextClassification(vocab_size = vocab_size, embed_dim = embedding_size, num_classes = num_classes).to(device)\n",
        "model"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TextClassification(\n",
              "  (embed): EmbeddingBag(95812, 64, mode=mean)\n",
              "  (fc): Linear(in_features=64, out_features=4, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "anxfwQLOMBp-"
      },
      "source": [
        "# Optimizer\n",
        "from torch.optim import AdamW, SGD\n",
        "\n",
        "optimizer = SGD(model.parameters(), lr = 5)\n",
        "\n",
        "# Criterion\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Obtaining train/test datasets\n",
        "train_data, test_data = AG_NEWS()\n",
        "\n",
        "train = DataLoader(list(train_data), shuffle = True, collate_fn = collate_batch, batch_size = 64)\n",
        "test = DataLoader(list(test_data), shuffle = True, collate_fn = collate_batch, batch_size = 64)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YThPZJrugRAk"
      },
      "source": [
        "### **1) Training Loop**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QmPC0FTmSsQv",
        "outputId": "c5b6e07e-7e04-4e03-8615-a102729df0e2"
      },
      "source": [
        "# Training loop\n",
        "%time\n",
        "\n",
        "# Putting model in train mode\n",
        "model.train()\n",
        "\n",
        "# Epochs \n",
        "epochs = 20\n",
        "\n",
        "for epoch in range(epochs):\n",
        "\n",
        "  # Calculating training accuracy for every epoch\n",
        "  total_acc, total_count = 0, 0\n",
        "\n",
        "  # Training batches\n",
        "  for (label, text, offsets) in train:\n",
        "    \n",
        "    # Push variables to device\n",
        "    #label, text, offsets = label.to(device), text.to(device), offsets.to(device)\n",
        "\n",
        "    # Clear out gradients from previous training batch\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Forward pass; feed inputs to model & get outputs\n",
        "    outputs = model(text, offsets)\n",
        "  \n",
        "    # Calculate loss between model's predictions & actual target\n",
        "    loss = criterion(outputs, label)\n",
        "\n",
        "    # Back propagate loss throughout the neural network\n",
        "    loss.backward()\n",
        "\n",
        "    # To prevent exploding gradients problem, clipping the norm of the gradients to 0.1\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
        "\n",
        "    # Update parameters based on the current gradient\n",
        "    optimizer.step()\n",
        "\n",
        "    total_acc += (outputs.argmax(1) == label).sum().item()\n",
        "    total_count += label.size(0)\n",
        "  \n",
        "  print('\\nEpoch {}/{}    -  Training Accuracy: {}'.format(epoch + 1, epochs, total_acc/ total_count))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 3 µs, sys: 0 ns, total: 3 µs\n",
            "Wall time: 5.48 µs\n",
            "\n",
            "Epoch 1/20    -  Training Accuracy: 0.8198\n",
            "\n",
            "Epoch 2/20    -  Training Accuracy: 0.9016083333333333\n",
            "\n",
            "Epoch 3/20    -  Training Accuracy: 0.9152333333333333\n",
            "\n",
            "Epoch 4/20    -  Training Accuracy: 0.9240333333333334\n",
            "\n",
            "Epoch 5/20    -  Training Accuracy: 0.9291083333333333\n",
            "\n",
            "Epoch 6/20    -  Training Accuracy: 0.9348666666666666\n",
            "\n",
            "Epoch 7/20    -  Training Accuracy: 0.9380333333333334\n",
            "\n",
            "Epoch 8/20    -  Training Accuracy: 0.9423416666666666\n",
            "\n",
            "Epoch 9/20    -  Training Accuracy: 0.945\n",
            "\n",
            "Epoch 10/20    -  Training Accuracy: 0.9481666666666667\n",
            "\n",
            "Epoch 11/20    -  Training Accuracy: 0.9507833333333333\n",
            "\n",
            "Epoch 12/20    -  Training Accuracy: 0.9532916666666666\n",
            "\n",
            "Epoch 13/20    -  Training Accuracy: 0.9548333333333333\n",
            "\n",
            "Epoch 14/20    -  Training Accuracy: 0.9573416666666666\n",
            "\n",
            "Epoch 15/20    -  Training Accuracy: 0.958825\n",
            "\n",
            "Epoch 16/20    -  Training Accuracy: 0.9605583333333333\n",
            "\n",
            "Epoch 17/20    -  Training Accuracy: 0.9624583333333333\n",
            "\n",
            "Epoch 18/20    -  Training Accuracy: 0.9638\n",
            "\n",
            "Epoch 19/20    -  Training Accuracy: 0.9650083333333334\n",
            "\n",
            "Epoch 20/20    -  Training Accuracy: 0.9674\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CuYHrevYgT_o"
      },
      "source": [
        "## **III. Testing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lgPTVBqGZ_TC",
        "outputId": "20920b3f-4fee-4a4f-9a54-453e0f60075e"
      },
      "source": [
        "# Testing\n",
        "# Putting model in eval mode\n",
        "model.eval()\n",
        "\n",
        "# Accuracy\n",
        "test_acc, test_count = 0, 0 \n",
        "\n",
        "# Disabling torch gradient calculation.\n",
        "with torch.no_grad():\n",
        "  # Iterating through batches in the test dataloader\n",
        "  for (label, text, offset) in test:\n",
        "    \n",
        "    # Feeding inputs to the model\n",
        "    output = model(text, offset)\n",
        "\n",
        "    # Identifying the correctly predicted labels\n",
        "    test_acc += (output.argmax(1) == label).sum().item()\n",
        "    test_count += label.size(0)\n",
        "\n",
        "\n",
        "# Test Accuracy\n",
        "print('Test Accuracy: ', test_acc/test_count)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Accuracy:  0.9006578947368421\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}