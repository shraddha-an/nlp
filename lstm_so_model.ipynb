{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lstm_so_model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNfK03zArFzU4PLD+LC7PhE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shraddha-an/nlp/blob/main/lstm_so_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UyuGKvXYur0G"
      },
      "source": [
        "# **Text Classification with fastText**\n",
        "\n",
        "This project is a continuation of my NLP Case Study looking at different models to classify the quality of Stack Overflow Questions.\n",
        "\n",
        "**Dataset**: **[Stack Overflow Questions](https://www.kaggle.com/imoore/60k-stack-overflow-questions-with-quality-rate)**\n",
        "\n",
        "Other models in this series:\n",
        "\n",
        "1. **[Training a Word Embedding](https://github.com/shraddha-an/nlp/blob/main/word_embedding_classification.ipynb)**\n",
        "2. **[Pre-trained GloVe Embedding](https://github.com/shraddha-an/nlp/blob/main/pretrained_glove_classification.ipynb)**\n",
        "3. **[fastText Classifier](https://github.com/shraddha-an/nlp/blob/main/so_fasttext.ipynb)**\n",
        "4. **[BERT Model](https://github.com/shraddha-an/nlp/blob/main/so_bert.ipynb)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQvNVYCYvacp"
      },
      "source": [
        "## **1) Data Preparation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3jAE2AWfsiSF"
      },
      "source": [
        "# Importing libraries\n",
        "# Data Manipulation/Handling\n",
        "import pandas as pd, numpy as np\n",
        "\n",
        "# NLP Preprocessing\n",
        "from gensim.utils import simple_preprocess"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cHnqIl1BxYge"
      },
      "source": [
        "# Importing data\n",
        "dataset = pd.read_csv('train.csv')[['Body', 'Y']].rename(columns = {'Body': 'questions', 'Y': 'category'})\n",
        "ds = pd.read_csv('valid.csv')[['Body', 'Y']].rename(columns = {'Body': 'questions', 'Y': 'category'})\n",
        "\n",
        "# Simple NLP Preprocessing\n",
        "X_train = dataset.iloc[:, 0].apply(lambda x: ' '.join(simple_preprocess(x)))\n",
        "X_test = ds.iloc[:, 0].apply(lambda x: ' '.join(simple_preprocess(x)))\n",
        "\n",
        "# Train/Test subsets\n",
        "y_train = pd.get_dummies(dataset[['category']])\n",
        "y_test = pd.get_dummies(ds[['category']])\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dO5FpE9qw8tW",
        "outputId": "cf75179d-e59a-47ce-f26c-058d99703e16",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(X_train.shape, '\\n', X_test.shape)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(45000,) \n",
            " (15000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fweMbnK4vi0Z"
      },
      "source": [
        "## **2) Tokenization**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DQkgCJexvnU7"
      },
      "source": [
        "# Setting the size of the vocabulary & sequence length for the embedding\n",
        "seq_len = 100\n",
        "vocab_size = 2100\n",
        "\n",
        "# Tokenization\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "tk = Tokenizer(num_words = vocab_size)\n",
        "tk.fit_on_texts(X_train)\n",
        "\n",
        "X_train_seq = tk.texts_to_sequences(X_train)\n",
        "X_test_seq = tk.texts_to_sequences(X_test)\n",
        "\n",
        "word_index = tk.word_index\n",
        "\n",
        "# Padding\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "X_train_seq = pad_sequences(X_train_seq, padding = 'post', maxlen = seq_len)\n",
        "X_test_seq = pad_sequences(X_test_seq, padding = 'post', maxlen = seq_len)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SSjwHVEowPQu"
      },
      "source": [
        "# **3) Training the LSTM Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fP4SFqsEwTTt",
        "outputId": "3a5337d0-5f85-48d5-baef-7eddd740f1e5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Embedding + LSTM\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, LSTM, Dense, Flatten\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim = vocab_size, output_dim = 8, input_length = seq_len))\n",
        "model.add(LSTM(units = 10, activation = 'tanh'))\n",
        "model.add(Dense(units = 3, activation = 'softmax'))\n",
        "model.compile(optimizer = 'adam', metrics = ['accuracy'], loss = 'categorical_crossentropy')\n",
        "model.summary()\n",
        "\n",
        "history = model.fit(X_train_seq, y_train, epochs = 10, batch_size = 512, verbose = 1)\n",
        "\n",
        "# Saving the model\n",
        "#model.save('saved_models/lstm_79.h5')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 100, 8)            16800     \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 10)                760       \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 3)                 33        \n",
            "=================================================================\n",
            "Total params: 17,593\n",
            "Trainable params: 17,593\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "88/88 [==============================] - 6s 64ms/step - loss: 1.0831 - accuracy: 0.4238\n",
            "Epoch 2/10\n",
            "88/88 [==============================] - 5s 62ms/step - loss: 0.9539 - accuracy: 0.5218\n",
            "Epoch 3/10\n",
            "88/88 [==============================] - 6s 63ms/step - loss: 0.8410 - accuracy: 0.5880\n",
            "Epoch 4/10\n",
            "88/88 [==============================] - 6s 63ms/step - loss: 0.7782 - accuracy: 0.6145\n",
            "Epoch 5/10\n",
            "88/88 [==============================] - 5s 62ms/step - loss: 0.7337 - accuracy: 0.6480\n",
            "Epoch 6/10\n",
            "88/88 [==============================] - 5s 62ms/step - loss: 0.6916 - accuracy: 0.6905\n",
            "Epoch 7/10\n",
            "88/88 [==============================] - 6s 63ms/step - loss: 0.6397 - accuracy: 0.7322\n",
            "Epoch 8/10\n",
            "88/88 [==============================] - 6s 63ms/step - loss: 0.5846 - accuracy: 0.7649\n",
            "Epoch 9/10\n",
            "88/88 [==============================] - 6s 63ms/step - loss: 0.5386 - accuracy: 0.7874\n",
            "Epoch 10/10\n",
            "88/88 [==============================] - 6s 63ms/step - loss: 0.5005 - accuracy: 0.8060\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dt1dzzF1wW-o"
      },
      "source": [
        "## **4) Evaluating Performance on Test Set**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nk31-mjswWde",
        "outputId": "658e43bf-23c6-4801-aed1-8773c124c836",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Evaluation\n",
        "loss, acc = model.evaluate(X_test_seq, y_test, verbose = 1)\n",
        "print('\\nAccuracy: {}\\nLoss: {}'.format(acc, loss))\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "469/469 [==============================] - 3s 6ms/step - loss: 0.5558 - accuracy: 0.7817\n",
            "\n",
            "Accuracy: 0.7816666960716248\n",
            "Loss: 0.5557811856269836\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}